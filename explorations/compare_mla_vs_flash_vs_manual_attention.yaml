# ── compare_manual-vs-flash-vs-mla.yaml ──────────────────────────────────────
# Base hyper-params that stay the same for every run
block_size: [1024]
n_layer:   [12]
n_head:    [12]
n_embd:    [768]
dataset:   ["minipile"]
device:    ["cuda"]
dtype:     ["float16", "bfloat16"]
max_iters: [20000]
eval_interval: [20000]

# Sweep across three *parameter-groups*
parameter_groups:

  # 1) Causal attention, **manual** SDPA (Flash disabled)
  - attention_variant:       ["causal"]
    disable_flash_attention: [true]

  # 2) Causal attention **with** Flash-Attention
  - attention_variant:       ["causal"]
    disable_flash_attention: [false]
    use_flash_lobo:          [true]
    use_flash_lobo_per_head: [true]
    flash_lobo_log_const:    [1.0, 0.0, -1.0]

  # 3) Multi-Latent Attention (MLA)
  - attention_variant: ["mla"]
    disable_flash_attention: [true]
    mla_latent_dim:         [64, 128, 256]   # d_c  – latent size
    mla_rotary_dim:         [32, 64]         # d_r  – rotary channels per head
    # ----- LoBo grid -----
    use_mla_lobo:        [true, false]
    mla_lobo_per_head:   [true]
    mla_lobo_init:       [1.0, 0.0, -1.0]   # log-space (~2.718, ~1.0  and  ~0.14)


# A couple of convenience flags that often vary run-to-run
seed:                       {range: {start: 0, end: 2, step: 1}}
compile:                    [true]
