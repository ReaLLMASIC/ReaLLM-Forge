# ðŸ§ª Managing Experiments & Sweeps

This guide provides a detailed overview of the automated experiment and hyperparameter search workflows available in the ReaLLM-Forge repository. These tools are essential for systematic exploration and optimization of model configurations.

---

## 1. Automated Experiment Sweeps (`run_experiments.py`)

The `optimization_and_search/run_experiments.py` script is designed to run a series of training experiments from a single configuration file. This is the primary method for systematically testing a predefined set of hyperparameters or architectural variations.

### Configuration Format

The script expects a JSON or YAML file containing a list of configurations. Each configuration in the list is a dictionary where the keys are command-line arguments for `train.py` and the values are their corresponding settings.

**Example `explorations/my_experiment.json`:**
```json
[
    {
        "dataset": "shakespeare_char",
        "n_layer": 4,
        "n_head": 4,
        "n_embd": 256,
        "out_dir": "out/exp1_small_model",
        "tensorboard_run_name": "exp1_small_model"
    },
    {
        "dataset": "shakespeare_char",
        "n_layer": 6,
        "n_head": 6,
        "n_embd": 384,
        "out_dir": "out/exp2_medium_model",
        "tensorboard_run_name": "exp2_medium_model"
    }
]
```

### How to Run

Execute the `run_experiments.py` script using the `-c` or `--config` flag to specify your configuration file.

```bash
python optimization_and_search/run_experiments.py -c explorations/my_experiment.json
```

The script will sequentially execute a `train.py` run for each configuration in the list.

### Monitoring with the Exploration Monitor (`run_exploration_monitor.py`)

While your experiment sweep is running (or after it has finished), you can use the TUI-based monitor to view results, sort, filter, and compare runs.

**How to Run:**
Point the monitor to the `results.yaml` file that is automatically generated by `run_experiments.py` in the same directory as your experiment's output.

```bash
python run_exploration_monitor.py <path_to_your_experiment_output>/results.yaml
```

**Key Features:**
-   **Live Refresh:** The monitor periodically reloads the log file to show results from new runs as they complete.
-   **Interactive Table:** Sort by any column, hide/show columns, and move them around.
-   **Filtering:** Hide or isolate rows based on specific values to focus on relevant runs.
-   **Plotting:** Generate matplotlib plots directly from the TUI to visualize relationships between hyperparameters and results.
-   **Exporting:** Export the current view of the data to a CSV file for further analysis.

---

## 2. Greedy Hyperparameter Search (`hyperparam_search.py`)

The `hyperparam_search.py` script implements a more advanced workflow for performing a greedy, iterative hyperparameter search. It starts from a baseline configuration and systematically explores the hyperparameter space to find the most efficient configuration based on a target metric (e.g., performance improvement per parameter added).

### Configuration

This script uses a `baseline.yaml` file to define the starting point for the search.

**Example `baseline.yaml`:**
```yaml
dataset: shakespeare_char
n_layer: 4
n_head: 4
n_embd: 256
batch_size: 32
learning_rate: 1e-3
max_iters: 5000
```

### How to Run

The search is configured and launched via a shell script (e.g., `run_hp_search.sh`).

**Example `run_hp_search.sh`:**
```bash
#!/bin/bash

python hyperparam_search.py \
  --orig_settings baseline.yaml \
  --param_names n_layer n_head n_embd \
  --increments 1 1 16 \
  --iterations 2 \
  --num_iterations 10 \
  --results_file my_hp_search_log.yaml
```

-   `--orig_settings`: The baseline configuration file.
-   `--param_names`: The hyperparameters to search over.
-   `--increments`: The step size to adjust each parameter by.
-   `--iterations`: The number of steps to take in each direction for each parameter.
-   `--num_iterations`: The total number of greedy search iterations.
-   `--results_file`: The YAML file to log the results of the search.

The script will run a series of training jobs, and the results will be logged to the specified YAML file. You can monitor this file to track the progress of the search.

---

## 3. Vizier Hyperparameter Search (`run_vizier.py`)

The `optimization_and_search/run_vizier.py` script integrates with Google's Vizier service for black-box optimization. This allows for more sophisticated search algorithms beyond simple grid or random search.

### Configuration Format

The Vizier script uses a JSON configuration file that defines the search space for each hyperparameter.

**Example `explorations/vizier_config.json`:**
```json
[
    {
        "dataset": "shakespeare_char",
        "learning_rate": {
            "range": {
                "start": 1e-4,
                "end": 1e-2,
                "step": 1e-4
            }
        },
        "n_layer": [4, 6, 8],
        "n_head": [4, 6, 8],
        "activation_variant": ["gelu", "relu", "silu"]
    }
]
```

-   **Ranges:** For floating-point or integer parameters, you can specify a `range` with `start`, `end`, and `step`.
-   **Categorical:** For string or boolean parameters, you can provide a list of `feasible_values`.

### How to Run

Execute the `run_vizier.py` script with your configuration file.

```bash
python optimization_and_search/run_vizier.py --config explorations/vizier_config.json
```

-   `--config`: Path to the Vizier configuration JSON file.
-   `--vizier_iterations`: The number of trials Vizier should run.
-   `--vizier_algorithm`: The Vizier search algorithm to use (e.g., `GAUSSIAN_PROCESS_BANDIT`, `RANDOM_SEARCH`).

The script will interact with the Vizier service to get suggestions for hyperparameter configurations, run training jobs with those configurations, and report the results back to Vizier.
